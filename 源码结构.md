# DETR源码结构

## 简易流程

1. [创建模型](main.py) `model, criterion, postprocessors = build_model(args)`
2. [加载数据集](main.py)
3. for epoch in epochs
    1. [train_one_epoch](engine.py)
        1. `for samples, targets in`
            1. `outputs=model(samples)`
                1. `features,pos = self.backbone(samples)`
                2. `hs=self.transformer`
                    1. query_embed
                    2. tgt
                    3. `memory=self.encoder`
                    4. `hs=self.decoder`
                3. `outputs_class=self.class_embed(hs)`
                4. `outputs_coord=self.bbox_embed(hs).sigmoid()`
            2. `loss_dict = criterion(outputs,targets)`
                1. `indices=self.matcher(outputs_without_aux, targets)`
                2. `get_loss`
                    1. self.loss_labels
                    2. self.loss_boxes

    2. 学习率更新
    3. [evaluate](main.py)

## 模型结构

```
  (class_embed): Linear(in_features=256, out_features=92, bias=True)
  (bbox_embed): MLP(
    (layers): ModuleList(
      (0): Linear(in_features=256, out_features=256, bias=True)
      (1): Linear(in_features=256, out_features=256, bias=True)
      (2): Linear(in_features=256, out_features=4, bias=True)
    )
  )
  (query_embed): Embedding(100, 256)
  (input_proj): Conv2d(2048, 256, kernel_size=(1, 1), stride=(1, 1))
  
  (0): TransformerEncoderLayer(
    (self_attn): MultiheadAttention(
      (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
    )
    (dropout1): Dropout(p=0.1, inplace=False)
    (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
    (linear1): Linear(in_features=256, out_features=2048, bias=True)
    (dropout): Dropout(p=0.1, inplace=False)
    (linear2): Linear(in_features=2048, out_features=256, bias=True)
    (dropout2): Dropout(p=0.1, inplace=False)
    (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
  )
        
        
  (0): TransformerDecoderLayer(
    (self_attn): MultiheadAttention(
      (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
    )
    (dropout1): Dropout(p=0.1, inplace=False)
    (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
    (multihead_attn): MultiheadAttention(
      (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
    )
    (dropout2): Dropout(p=0.1, inplace=False)
    (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
    (linear1): Linear(in_features=256, out_features=2048, bias=True)
    (dropout): Dropout(p=0.1, inplace=False)
    (linear2): Linear(in_features=2048, out_features=256, bias=True)
    (dropout3): Dropout(p=0.1, inplace=False)
    (norm3): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
  )
```

## Loss内容

loss_ce: 2.223 loss_bbox: 2.227 loss_giou: 1.771  
loss_ce_0: 2.079 loss_bbox_0: 2.507 loss_giou_0: 1.828  
loss_ce_1: 2.097 loss_bbox_1: 2.436 loss_giou_1: 1.852  
loss_ce_2: 2.129 loss_bbox_2: 2.462 loss_giou_2: 1.749  
loss_ce_3: 2.149 loss_bbox_3: 2.38 loss_giou_3: 1.718  
loss_ce_4: 2.128 loss_bbox_4: 2.214 loss_giou_4: 1.712  